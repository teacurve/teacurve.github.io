<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://teacurve.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://teacurve.github.io/" rel="alternate" type="text/html" /><updated>2021-08-01T20:42:32+00:00</updated><id>https://teacurve.github.io/feed.xml</id><title type="html">Teacurve</title><subtitle>Teacurve</subtitle><entry><title type="html">Proxmox Automation Part 2: Automate Salt master creation with cloud-init and Terraform</title><link href="https://teacurve.github.io/2021/08/01/proxmox-automation-part-2-salt-master-creation.html" rel="alternate" type="text/html" title="Proxmox Automation Part 2: Automate Salt master creation with cloud-init and Terraform" /><published>2021-08-01T16:30:00+00:00</published><updated>2021-08-01T16:30:00+00:00</updated><id>https://teacurve.github.io/2021/08/01/proxmox-automation-part-2-salt-master-creation</id><content type="html" xml:base="https://teacurve.github.io/2021/08/01/proxmox-automation-part-2-salt-master-creation.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&quot;proxmox-automation-part-1-terraform&quot;&gt;part 1&lt;/a&gt; of this Proxmox Automation series, I showed how to configure a Ubuntu Server cloud-ready template, and create a new instance on Proxmox using cloud-init and Terraform.&lt;/p&gt;

&lt;p&gt;The first post was the simple way - directly using Proxmox cloud-init functionality to customize the bare minimum parameters required to start a new instance; namely setting an authorized ssh key, and setting the network IP (or in our case, setting it to use DHCP).&lt;/p&gt;

&lt;p&gt;However, the real power of cloud-init happens when you define your own configuration file. Proxmox can generate the &lt;a href=&quot;https://cloudinit.readthedocs.io/en/latest/topics/datasources/nocloud.html&quot;&gt;magical cd-rom image&lt;/a&gt; for cloud-init based on the contents of this file, which lets you specify lots more parameters to customize the new instance.&lt;/p&gt;

&lt;p&gt;In this post, I’ll take advantage of this extra functionality to configure a salt master node.&lt;/p&gt;

&lt;p&gt;SaltStack (Salt) is a configuration management system, like Puppet, Chef or Ansible. It is again part of the ‘infrastructure as code’ revolution, but focusses on software installation and configuration. This is as compared to something like Terraform, which concerns itself with creation and configuration of the (physical or virtual) hardware itself.&lt;/p&gt;

&lt;p&gt;Simply put; I’ll use Terraform to create VMs on Proxmox, and Salt to install software, and configure services.&lt;/p&gt;

&lt;p&gt;Cloud-init does let you install software, and there is some overlap here. But cloud-init is really for initial-state configuration, Salt lets you manage the instance through its full lifecycle, and has more flexible and powerful tools to help you do that. Of course, the chicken-egg situation presents itself here; we need to somehow install Salt so that we can install further software. That’s where cloud-init comes in; the plan is to install salt with cloud-init, then let Salt take care of the rest.&lt;/p&gt;

&lt;h2 id=&quot;salt-basics&quot;&gt;Salt Basics&lt;/h2&gt;

&lt;h3 id=&quot;why-salt&quot;&gt;Why salt&lt;/h3&gt;

&lt;p&gt;There are four big names in configuration management;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Chef&lt;/li&gt;
  &lt;li&gt;Ansible&lt;/li&gt;
  &lt;li&gt;Puppet&lt;/li&gt;
  &lt;li&gt;Salt&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Of these, I previously used ansible. I wanted to try out Salt because A) it’s something new to try, and B) it claims to be faster, and Ansible has been quite slow for me (probably due to it operating over SSH).&lt;/p&gt;

&lt;p&gt;There are multiple good articles comparing the different systems, all of which of course have their own pros and cons. After doing some research, I settled on Salt, for a bunch of reasons.&lt;/p&gt;

&lt;p&gt;I do value the ability of Ansible to operate over SSH (agentless), but you have to create the image in the right way for it to happen (for example a sudo NOPASSWD user). Since we can automate  the creation of the Salt Master with Terraform anyway, it seemed like it was worth biting the bullet and trying it out. Additionally, it is possible to configure Salt to run &lt;a href=&quot;https://docs.saltproject.io/en/latest/topics/tutorials/quickstart.html&quot; target=&quot;_blank&quot;&gt;masterless&lt;/a&gt;, if you so choose.&lt;/p&gt;

&lt;p&gt;Additionally, I already use a single server (VM) to do most of my administration from - which I ssh into. Doing it from Making this a salt server&lt;/p&gt;

&lt;h3 id=&quot;salt-masterminion&quot;&gt;Salt Master/Minion&lt;/h3&gt;

&lt;p&gt;As previously mentioned, Salt can operate in a masterless mode, but we’ll be automating the creation of a salt master node in this article.&lt;/p&gt;

&lt;p&gt;The salt master node is responsible for handing out commands to all the minions. Communication happens over a custom (encrypted, ZeroMQ) protocol.&lt;/p&gt;

&lt;p&gt;Salt execution modules are python modules that are run on a salt minion. There are many built in, many third party, and you can &lt;a href=&quot;https://www.linode.com/docs/guides/create-a-salt-execution-module/&quot; target=&quot;_blank&quot;&gt;write your own&lt;/a&gt;, which makes this a really powerful configuration mechanism. Especially since it’s not Ruby…&lt;/p&gt;

&lt;p&gt;Salt states are configuration files that specify what a node should have installed on it. Salt will then ‘make it so’. I think of this as saying ‘I wish I had a server that looked like this’ and then Salt makes the wish come true. This is also the same concept as Terraform - you describe how you wish the system to look, instead of describing the actions it should take (kind of).&lt;/p&gt;

&lt;h2 id=&quot;terraform-cloud-init-revisited&quot;&gt;Terraform cloud-init revisited&lt;/h2&gt;

&lt;p&gt;We have previously used the special proxmox variables to configure basic-cloud-init settings. However, cloud-init works with a powerful config file that has many modules that allow for specifying all sorts of initial configuration, including package installation, and service configuration. This is all documented &lt;a href=&quot;https://cloudinit.readthedocs.io/en/latest/topics/modules.html&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. Proxmox allows you to specify this file (which is passed to the vm via the magic cd-rom) as a ‘snippet’, which is what we shall use.&lt;/p&gt;

&lt;p&gt;First, make a new terraform module to work from. You can copy the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf&lt;/code&gt; files from the previous post:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;manager@linux-manager:~/terraform$ mkdir saltmaster-tf
manager@linux-manager:~/terraform$ cp terraform/main.tf saltmaster-tf/
manager@linux-manager:~/terraform$ cp terraform/vm.tf saltmaster-tf/
manager@linux-manager:~/terraform$ cd saltmaster-tf/
manager@linux-manager:~/terraform/saltmaster-tf$ terraform init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;reproduce-post1-with-a-cloud-config-file&quot;&gt;Reproduce post1 with a cloud config file&lt;/h3&gt;

&lt;p&gt;I like to do things iteratively, so the first part of this will be just getting an instance up and running as in &lt;a href=&quot;proxmox-automation-part-1-terraform&quot;&gt;Part 1&lt;/a&gt;, but passing the image customizations in a config file instead of using Proxmox’s special variables. Namely, we want to configure an authorized SSH key (we’ll continue to set the network ip as dhcp via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ipconfig0&lt;/code&gt; parameter).&lt;/p&gt;

&lt;h4 id=&quot;enable-snippets&quot;&gt;Enable snippets&lt;/h4&gt;

&lt;p&gt;The Proxmox cloud-init config file works by having a local file on the proxmox server, which is a ‘snippet’. You first need to enable snippets on the local directory of the proxmox server. Go to Database -&amp;gt; Storage -&amp;gt; Local -&amp;gt; Edit, click on Content and add ‘Snippets’. You may also need to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkdir snippets&lt;/code&gt; in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/lib/vz/&lt;/code&gt; directory on the proxmox server.&lt;/p&gt;

&lt;h4 id=&quot;terraform-module&quot;&gt;Terraform module&lt;/h4&gt;

&lt;p&gt;For convenience, here is the unchanged &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.tf&lt;/code&gt; file that contains the provider configuration:”&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# PM_API_TOKEN_SECRET 
# PM_API_TOKEN_ID 
# (in .bashrc)

terraform {
  required_providers {
    proxmox = {
      source = &quot;telmate/proxmox&quot;
      version = &quot;2.7.4&quot;
    }
  }
}


provider &quot;proxmox&quot; {
  pm_api_url = &quot;https://192.168.1.10:8006/api2/json&quot;
  pm_tls_insecure = &quot;true&quot;
  pm_log_enable = true
  pm_log_file = &quot;terraform-plugin-proxmox.log&quot;
  pm_log_levels = {
    _default = &quot;debug&quot;
    _capturelog = &quot;&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the full &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vm.tf&lt;/code&gt; file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
locals {
  cloud_init_name = &quot;ubuntu-server-focal-saltmaster-cloud-init&quot;
  cloud_init_template_fn = &quot;${local.cloud_init_name}.tpl&quot;
  cloud_init_fn = &quot;${local.cloud_init_name}.yml&quot; 
  proxmox_ip = &quot;192.168.1.10&quot;
  template_file_init = templatefile(&quot;${path.module}/files/${local.cloud_init_template_fn}&quot;, {
    ssh_key = file(&quot;~/.ssh/id_rsa.pub&quot;)
  })
}

resource &quot;local_file&quot; &quot;saltmaster-cloud-init-local&quot; {
  content = local.template_file_init
  filename = &quot;${path.module}/files/${local.cloud_init_fn}&quot;
}


# this is the dirty bit; we need to be able to put the file to the proxmox server, 
# so we use SSH
resource &quot;null_resource&quot; &quot;cloud_init_salt_master_config&quot; {
  connection {
    type    = &quot;ssh&quot;
    user    = &quot;root&quot;
    private_key = file(&quot;~/.ssh/id_rsa&quot;)
    host    = &quot;${local.proxmox_ip}&quot;
  }

  provisioner &quot;file&quot; {
    source       = local_file.saltmaster-cloud-init-local.filename
    destination  = &quot;/var/lib/vz/snippets/${local.cloud_init_fn}&quot;
  }
}


resource &quot;proxmox_vm_qemu&quot; &quot;salt-master-01&quot; {

  depends_on = [
    null_resource.cloud_init_salt_master_config
  ]

  name = &quot;salt-master-01&quot;
  target_node = &quot;hades&quot;
  clone = &quot;ubuntu-cloudready-template&quot;
  os_type = &quot;cloud-init&quot;
  balloon = 1024
  boot = &quot;order=scsi0&quot;

  agent = 1

  cores = 2
  sockets = 1
  memory = 2560
  
  disk {  
      size            = &quot;10G&quot;
      type            = &quot;scsi&quot;
      storage         = &quot;local-lvm&quot;
  }
  
  vga {
    type = &quot;std&quot;
  }


  # Set the network
  network {
    model = &quot;virtio&quot;
    bridge = &quot;vmbr0&quot;
  }

  # We record which user to SSH in as
  ssh_user = &quot;ubuntu&quot;


  # The cloud init variables
  ipconfig0 = &quot;ip=dhcp&quot;
  cicustom = &quot;user=local:snippets/${local.cloud_init_fn}&quot;

  # Ignore changes to the network
  ## MAC address is generated on every apply, causing
  ## TF to think this needs to be rebuilt on every apply
  lifecycle {
      ignore_changes = [
          network
      ]
  }

  connection {
      type = &quot;ssh&quot;
      user = &quot;${self.ssh_user}&quot;
      private_key = &quot;${file(&quot;~/.ssh/id_rsa&quot;)}&quot;
      host = &quot;${self.ssh_host}&quot;
      port = &quot;${self.ssh_port}&quot;
  }

  provisioner &quot;remote-exec&quot; {
    inline = [  
      // here you will see the actual ip address        
      &quot;/sbin/ip a&quot;  
    ]      
  }

}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also need a place to store the config file template, so:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkdir files&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;And inside there, I have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu-server-focal-saltmaster-cloud-init.tpl&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#cloud-config
users:
- name: ubuntu
  ssh_authorized_keys:
    - ${ssh_key}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;terraform-module-explained&quot;&gt;Terraform module explained&lt;/h4&gt;

&lt;p&gt;Most of this is the same as before. At the start of the file, we have a locals block:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;locals {
  cloud_init_name = &quot;ubuntu-server-focal-saltmaster-cloud-init&quot;
  cloud_init_template_fn = &quot;${local.cloud_init_name}.tpl&quot;
  cloud_init_fn = &quot;${local.cloud_init_name}.yml&quot; 
  proxmox_ip = &quot;192.168.1.10&quot;
  template_file_init = templatefile(&quot;${path.module}/files/${local.cloud_init_template_fn}&quot;, {
    ssh_key = file(&quot;~/.ssh/id_rsa.pub&quot;)
  })
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here I define some local variables, including the names of the files that I’m using. Additionally, I use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templatefile&lt;/code&gt; function to render my template cloud-init config. The config is very minimal, but shows how to use a templated argument (${ssh_key}). This is populated in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;templatefile&lt;/code&gt; function wih the parameter passed to it, which is my ssh public key file. The result is stored in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;template_file_init&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Now, since we have to put the config file as a snippet on the proxmox server, I write the result of the template expansion into a local file with the next block:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;resource &quot;local_file&quot; &quot;saltmaster-cloud-init-local&quot; {
  content = local.template_file_init
  filename = &quot;${path.module}/files/${local.cloud_init_fn}&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will then get put on the Proxmox server via SSH:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;resource &quot;null_resource&quot; &quot;cloud_init_salt_master_config&quot; {
  connection {
    type    = &quot;ssh&quot;
    user    = &quot;root&quot;
    private_key = file(&quot;~/.ssh/id_rsa&quot;)
    host    = &quot;${local.proxmox_ip}&quot;
  }

  provisioner &quot;file&quot; {
    source       = local_file.saltmaster-cloud-init-local.filename
    destination  = &quot;/var/lib/vz/snippets/${local.cloud_init_fn}&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We use a null resource to do the SSH transfer with the file provisioner. We define the parameters for the SSH connection - and for this I have already put my public key into the proxmox authorized_keys file. You can do that with a one-liner:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh root@proxmoxip &quot;mkdir ~/.ssh; echo \&quot;`cat ~/.ssh/^C_rsa.pub`\&quot; &amp;gt;&amp;gt; .ssh/authorized_keys&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally we declare our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;proxmox_vm_qemu&lt;/code&gt; resource as before. There are a few differences, but it’s mostly the same. We call it salt-master-01, and set the name to be the same.&lt;/p&gt;

&lt;p&gt;The first block:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;depends_on = [
null_resource.cloud_init_salt_master_config
]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;tells terraform to make sure the config file has been copied over before doing this resource, which is important.&lt;/p&gt;

&lt;p&gt;The final change is to the cloud-init parameters:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# The cloud init variables
ipconfig0 = &quot;ip=dhcp&quot;
cicustom = &quot;user=local:snippets/${local.cloud_init_fn}&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We no longer set the ssh_key here, but instead set the name of the cloud-init config file (which contains the ssh key)&lt;/p&gt;

&lt;h4 id=&quot;terraform-apply&quot;&gt;Terraform apply&lt;/h4&gt;

&lt;p&gt;Finally, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terraform plan&lt;/code&gt; to make sure the file is valid and doesn’t contain any typos. Once you’ve ensured it’s correct you can run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terraform apply&lt;/code&gt; to create the instance. Once complete, you should be able to ssh in via public key as before.&lt;/p&gt;

&lt;p&gt;At this point, it might not seem like we have done much, since we have just replicated what we were able to do before, with added complexity. However, we’ve laid the foundations for much more powerful cloud-init configuration, and now we will take advantage of that to set up our salt master node.&lt;/p&gt;

&lt;h2 id=&quot;create-the-salt-master&quot;&gt;Create the Salt master&lt;/h2&gt;

&lt;h3 id=&quot;terraform-module-1&quot;&gt;Terraform module&lt;/h3&gt;

&lt;p&gt;The main difference here is the cloud-init configuration file; the terraform module at this point is almost exactly the same. The only difference is that we have more templated arguments to patch in to our more complicated cloud-init config. The full new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vm.tf&lt;/code&gt; is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
locals {
  cloud_init_name = &quot;ubuntu-server-focal-saltmaster-cloud-init&quot;
  cloud_init_template_fn = &quot;${local.cloud_init_name}.tpl&quot;
  cloud_init_fn = &quot;${local.cloud_init_name}.yml&quot; 
  proxmox_ip = &quot;192.168.1.10&quot;
  salt_master_address = &quot;192.168.1.18&quot;
  template_file_init = templatefile(&quot;${path.module}/files/${local.cloud_init_template_fn}&quot;, {
    ssh_key = file(&quot;~/.ssh/id_rsa.pub&quot;)
    hostname = &quot;salt-master-01&quot;
    salt_master_address = &quot;${local.salt_master_address}&quot;
  })
}

resource &quot;local_file&quot; &quot;saltmaster-cloud-init-local&quot; {
  content = local.template_file_init
  filename = &quot;${path.module}/files/${local.cloud_init_fn}&quot;
}


# this is the dirty bit; we need to be able to put the file to the proxmox server, 
# so we use SSH
resource &quot;null_resource&quot; &quot;cloud_init_salt_master_config&quot; {
  connection {
    type    = &quot;ssh&quot;
    user    = &quot;root&quot;
    private_key = file(&quot;~/.ssh/id_rsa&quot;)
    host    = &quot;${local.proxmox_ip}&quot;
  }

  provisioner &quot;file&quot; {
    source       = local_file.saltmaster-cloud-init-local.filename
    destination  = &quot;/var/lib/vz/snippets/${local.cloud_init_fn}&quot;
  }
}


resource &quot;proxmox_vm_qemu&quot; &quot;salt-master-01&quot; {

  depends_on = [
    null_resource.cloud_init_salt_master_config
  ]

  name = &quot;salt-master-01&quot;
  target_node = &quot;hades&quot;
  clone = &quot;ubuntu-cloudready-template&quot;
  os_type = &quot;cloud-init&quot;
  balloon = 1024
  boot = &quot;order=scsi0&quot;

  agent = 1

  cores = 2
  sockets = 1
  memory = 2560
  
  disk {  
      size            = &quot;10G&quot;
      type            = &quot;scsi&quot;
      storage         = &quot;local-lvm&quot;
  }
  
  vga {
    type = &quot;std&quot;
  }


  # Set the network
  network {
    model = &quot;virtio&quot;
    bridge = &quot;vmbr0&quot;
  }

  # We record which user to SSH in as
  ssh_user = &quot;teacurve&quot;


  # The cloud init variables
  ipconfig0 = &quot;ip=${local.salt_master_address}/24,gw=192.168.1.1&quot;
  nameserver = &quot;192.168.1.8&quot;
  searchdomain = &quot;teanet.local&quot;
  cicustom = &quot;user=local:snippets/${local.cloud_init_fn}&quot;

  # Ignore changes to the network
  ## MAC address is generated on every apply, causing
  ## TF to think this needs to be rebuilt on every apply
  lifecycle {
      ignore_changes = [
          network
      ]
  }

  connection {
      type = &quot;ssh&quot;
      user = &quot;${self.ssh_user}&quot;
      private_key = &quot;${file(&quot;~/.ssh/id_rsa&quot;)}&quot;
      host = &quot;${self.ssh_host}&quot;
      port = &quot;${self.ssh_port}&quot;
  }

  provisioner &quot;remote-exec&quot; {
    inline = [  
      // here you will see the actual ip address        
      &quot;/sbin/ip a&quot;
    ]      
  }

}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the full cloud-init config file is here:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#cloud-config
# Add/configure the main user
users:
- name: teacurve
  groups: sudo
  shell: /bin/bash
  sudo: ['ALL=(ALL) NOPASSWD:ALL']
  ssh_authorized_keys:
    - ${ssh_key}

# set the hostname
fqdn: ${hostname}.teanet.local

# set some qol stuff
locale: en_US.UTF-8
timezone: America/New_York

# update machine
package_update: true
package_upgrade: true

apt:
  sources:
    saltstack.list:
      source: &quot;deb https://repo.saltproject.io/py3/ubuntu/20.04/amd64/latest focal main&quot;
      filename: saltstack.list
      key: |
        -----BEGIN PGP PUBLIC KEY BLOCK-----
        Version: GnuPG v2

        mQENBFOpvpgBCADkP656H41i8fpplEEB8IeLhugyC2rTEwwSclb8tQNYtUiGdna9
        m38kb0OS2DDrEdtdQb2hWCnswxaAkUunb2qq18vd3dBvlnI+C4/xu5ksZZkRj+fW
        tArNR18V+2jkwcG26m8AxIrT+m4M6/bgnSfHTBtT5adNfVcTHqiT1JtCbQcXmwVw
        WbqS6v/LhcsBE//SHne4uBCK/GHxZHhQ5jz5h+3vWeV4gvxS3Xu6v1IlIpLDwUts
        kT1DumfynYnnZmWTGc6SYyIFXTPJLtnoWDb9OBdWgZxXfHEcBsKGha+bXO+m2tHA
        gNneN9i5f8oNxo5njrL8jkCckOpNpng18BKXABEBAAG0MlNhbHRTdGFjayBQYWNr
        YWdpbmcgVGVhbSA8cGFja2FnaW5nQHNhbHRzdGFjay5jb20+iQE4BBMBAgAiBQJT
        qb6YAhsDBgsJCAcDAgYVCAIJCgsEFgIDAQIeAQIXgAAKCRAOCKFJ3le/vhkqB/0Q
        WzELZf4d87WApzolLG+zpsJKtt/ueXL1W1KA7JILhXB1uyvVORt8uA9FjmE083o1
        yE66wCya7V8hjNn2lkLXboOUd1UTErlRg1GYbIt++VPscTxHxwpjDGxDB1/fiX2o
        nK5SEpuj4IeIPJVE/uLNAwZyfX8DArLVJ5h8lknwiHlQLGlnOu9ulEAejwAKt9CU
        4oYTszYM4xrbtjB/fR+mPnYh2fBoQO4d/NQiejIEyd9IEEMd/03AJQBuMux62tjA
        /NwvQ9eqNgLw9NisFNHRWtP4jhAOsshv1WW+zPzu3ozoO+lLHixUIz7fqRk38q8Q
        9oNR31KvrkSNrFbA3D89uQENBFOpvpgBCADJ79iH10AfAfpTBEQwa6vzUI3Eltqb
        9aZ0xbZV8V/8pnuU7rqM7Z+nJgldibFk4gFG2bHCG1C5aEH/FmcOMvTKDhJSFQUx
        uhgxttMArXm2c22OSy1hpsnVG68G32Nag/QFEJ++3hNnbyGZpHnPiYgej3FrerQJ
        zv456wIsxRDMvJ1NZQB3twoCqwapC6FJE2hukSdWB5yCYpWlZJXBKzlYz/gwD/Fr
        GL578WrLhKw3UvnJmlpqQaDKwmV2s7MsoZogC6wkHE92kGPG2GmoRD3ALjmCvN1E
        PsIsQGnwpcXsRpYVCoW7e2nW4wUf7IkFZ94yOCmUq6WreWI4NggRcFC5ABEBAAGJ
        AR8EGAECAAkFAlOpvpgCGwwACgkQDgihSd5Xv74/NggA08kEdBkiWWwJZUZEy7cK
        WWcgjnRuOHd4rPeT+vQbOWGu6x4bxuVf9aTiYkf7ZjVF2lPn97EXOEGFWPZeZbH4
        vdRFH9jMtP+rrLt6+3c9j0M8SIJYwBL1+CNpEC/BuHj/Ra/cmnG5ZNhYebm76h5f
        T9iPW9fFww36FzFka4VPlvA4oB7ebBtquFg3sdQNU/MmTVV4jPFWXxh4oRDDR+8N
        1bcPnbB11b5ary99F/mqr7RgQ+YFF0uKRE3SKa7a+6cIuHEZ7Za+zhPaQlzAOZlx
        fuBmScum8uQTrEF5+Um5zkwC7EXTdH1co/+/V/fpOtxIg4XO4kcugZefVm5ERfVS
        MA==
        =dtMN
        -----END PGP PUBLIC KEY BLOCK-----

packages:
  - salt-master
  - tmux
  - vim


salt_minion:
  conf: 
    master: ${salt_master_address}
    id: ${hostname}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;terraform-module-explained-1&quot;&gt;Terraform module explained&lt;/h3&gt;

&lt;p&gt;The only small changes to the terraform are that firstly we changed the ssh_user to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;teacurve&lt;/code&gt; instead of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt;, since we’re now pushing our public ssh key to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;teacurve&lt;/code&gt; user instead. We’ve added a couple of new local variables useful for the new salt configuration.&lt;/p&gt;

&lt;p&gt;Additionally, we change the ip configuration from dhcp to a static ip, since we’ll need to know where this server is. Alternatively, we could rely on DNS. We use the same variable as in the config templatefile. Since we are setting a static IP, we also provide a nameserver and searchdomain which it would normally get via DHCP.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# The cloud init variables
ipconfig0 = &quot;ip=${local.salt_master_address}/24,gw=192.168.1.1&quot;
nameserver = &quot;192.168.1.8&quot;
searchdomain = &quot;teanet.local&quot;
cicustom = &quot;user=local:snippets/${local.cloud_init_fn}&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Moving on to the new cloud-init config file; first it sets up my real user (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;teacurve&lt;/code&gt;), sets its ssh key, and gives it passwordless sudo:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;users:
- name: teacurve
  groups: sudo
  shell: /bin/bash
  sudo: ['ALL=(ALL) NOPASSWD:ALL']
  ssh_authorized_keys:
    - ${ssh_key}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It should be noted that since I’m not pushing the ssh key for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt; default user, I will not be able to SSH in as that user.&lt;/p&gt;

&lt;p&gt;Next, we set the hostname of the new machine with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fqdn: ${hostname}.teanet.local
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is a variable, so it is actually set by the templatefile function in the terraform script.&lt;/p&gt;

&lt;p&gt;After that we set the locale/timezone and update the packages on the system. We also add a new repository source for apt to pull down the latest salt pacakges, complete with the public key required.&lt;/p&gt;

&lt;p&gt;After that, we install some required packages:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;packages:
  - salt-master
  - tmux
  - vim
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;salt-master&lt;/code&gt; will install the salt master on this node (which will be started automatically by default). I also install tmux and vim so that we can more easily interact with the server, though future packages should be installed by salt itself.&lt;/p&gt;

&lt;p&gt;Finally, we want to install and configure the master to also be its own minion, so it can manage itself. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cloud-init&lt;/code&gt; has special support for setting up a salt minion, documentation &lt;a href=&quot;https://cloudinit.readthedocs.io/en/latest/topics/modules.html#salt-minion&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. The main thing we want to do is direct the minion to the master (in this case, itself).&lt;/p&gt;

&lt;p&gt;The salt minion configuration is here:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;salt_minion:
  conf: 
    master: ${salt_master_address}
    id: ${hostname}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Having the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;salt_minion&lt;/code&gt; module entry present will ensure the salt minion software is installed; we don’t need to explicitly install it in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;packages&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We set the master target (which happens to be the same host). We also explicitly set the minion_id as the hostname. Normally, salt will generate the minion_id from the hostname, and cache it in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/salt/minion_id&lt;/code&gt;. I found that it was picking up an old hostname (‘ubuntu’ - from the template), so I needed to explicitly set it.&lt;/p&gt;

&lt;h3 id=&quot;instantiate-the-salt-master&quot;&gt;Instantiate the salt master&lt;/h3&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terraform apply -auto-approve&lt;/code&gt; to ignore the prompt, and once that’s finished, you can SSH in with your own user, and see that salt has indeed been installed:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;teacurve@salt-master-01:~$ apt list --installed | grep salt

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

salt-common/unknown,now 3003.1+ds-1 all [installed,automatic]
salt-master/unknown,now 3003.1+ds-1 all [installed]
salt-minion/unknown,now 3003.1+ds-1 all [installed]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;create-the-salt-minion&quot;&gt;Create the salt minion&lt;/h2&gt;

&lt;p&gt;Make a new directory for a new terraform module for the minion. Copy the tf files from the salt master, and also copy the .tpl file, but rename it to saltminion. You should end up with a directory structure like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;manager@linux-manager:~/terraform/saltminion-tf$ ls -R
.:
files  main.tf  vm.tf

./files:
ubuntu-server-focal-saltminion-cloud-init.tpl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;terraform-module-2&quot;&gt;Terraform module&lt;/h3&gt;

&lt;p&gt;Here’s the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vm.tf&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
locals {
  cloud_init_name = &quot;ubuntu-server-focal-saltminion-cloud-init&quot;
  cloud_init_template_fn = &quot;${local.cloud_init_name}.tpl&quot;
  cloud_init_fn = &quot;${local.cloud_init_name}.yml&quot; 
  proxmox_ip = &quot;192.168.1.10&quot;
  salt_master_address = &quot;192.168.1.18&quot;
  template_file_init = templatefile(&quot;${path.module}/files/${local.cloud_init_template_fn}&quot;, {
    ssh_key = file(&quot;~/.ssh/id_rsa.pub&quot;)
    hostname = &quot;salt-minion-01&quot;
    salt_master_address = &quot;${local.salt_master_address}&quot;
  })
}

resource &quot;local_file&quot; &quot;saltminion-cloud-init-local&quot; {
  content = local.template_file_init
  filename = &quot;${path.module}/files/${local.cloud_init_fn}&quot;
}


# this is the dirty bit; we need to be able to put the file to the proxmox server, 
# so we use SSH
resource &quot;null_resource&quot; &quot;cloud_init_salt_master_config&quot; {
  connection {
    type    = &quot;ssh&quot;
    user    = &quot;root&quot;
    private_key = file(&quot;~/.ssh/id_rsa&quot;)
    host    = &quot;${local.proxmox_ip}&quot;
  }

  provisioner &quot;file&quot; {
    source       = local_file.saltminion-cloud-init-local.filename
    destination  = &quot;/var/lib/vz/snippets/${local.cloud_init_fn}&quot;
  }
}


resource &quot;proxmox_vm_qemu&quot; &quot;salt-minion-01&quot; {

  depends_on = [
    null_resource.cloud_init_salt_master_config
  ]

  name = &quot;salt-minion-01&quot;
  target_node = &quot;hades&quot;
  clone = &quot;ubuntu-cloudready-template&quot;
  os_type = &quot;cloud-init&quot;
  balloon = 1024
  boot = &quot;order=scsi0&quot;

  agent = 1

  cores = 2
  sockets = 1
  memory = 2560
  
  disk {  
      size            = &quot;10G&quot;
      type            = &quot;scsi&quot;
      storage         = &quot;local-lvm&quot;
  }
  
  vga {
    type = &quot;std&quot;
  }


  # Set the network
  network {
    model = &quot;virtio&quot;
    bridge = &quot;vmbr0&quot;
  }

  # We record which user to SSH in as
  ssh_user = &quot;teacurve&quot;


  # The cloud init variables
  ipconfig0 = &quot;ip=dhcp&quot;
  cicustom = &quot;user=local:snippets/${local.cloud_init_fn}&quot;

  # Ignore changes to the network
  ## MAC address is generated on every apply, causing
  ## TF to think this needs to be rebuilt on every apply
  lifecycle {
      ignore_changes = [
          network
      ]
  }

  connection {
      type = &quot;ssh&quot;
      user = &quot;${self.ssh_user}&quot;
      private_key = &quot;${file(&quot;~/.ssh/id_rsa&quot;)}&quot;
      host = &quot;${self.ssh_host}&quot;
      port = &quot;${self.ssh_port}&quot;
  }

  provisioner &quot;remote-exec&quot; {
    inline = [  
      // here you will see the actual ip address        
      &quot;/sbin/ip a&quot;      
    ]      
  }

}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And the new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu-server-focal-saltminion-cloud-init.tpl&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#cloud-config
# Add/configure the main user
users:
- name: teacurve
  groups: sudo
  shell: /bin/bash
  sudo: ['ALL=(ALL) NOPASSWD:ALL']
  ssh_authorized_keys:
    - ${ssh_key}

# set the hostname
prefer_fqdn_over_hostname: true
fqdn: ${hostname}.teanet.local

# set some qol stuff
locale: en_US.UTF-8
timezone: America/New_York

# update machine
package_update: true
package_upgrade: true

apt:
  sources:
    saltstack.list:
      source: &quot;deb https://repo.saltproject.io/py3/ubuntu/20.04/amd64/latest focal main&quot;
      filename: saltstack.list
      key: |
        -----BEGIN PGP PUBLIC KEY BLOCK-----
        Version: GnuPG v2

        mQENBFOpvpgBCADkP656H41i8fpplEEB8IeLhugyC2rTEwwSclb8tQNYtUiGdna9
        m38kb0OS2DDrEdtdQb2hWCnswxaAkUunb2qq18vd3dBvlnI+C4/xu5ksZZkRj+fW
        tArNR18V+2jkwcG26m8AxIrT+m4M6/bgnSfHTBtT5adNfVcTHqiT1JtCbQcXmwVw
        WbqS6v/LhcsBE//SHne4uBCK/GHxZHhQ5jz5h+3vWeV4gvxS3Xu6v1IlIpLDwUts
        kT1DumfynYnnZmWTGc6SYyIFXTPJLtnoWDb9OBdWgZxXfHEcBsKGha+bXO+m2tHA
        gNneN9i5f8oNxo5njrL8jkCckOpNpng18BKXABEBAAG0MlNhbHRTdGFjayBQYWNr
        YWdpbmcgVGVhbSA8cGFja2FnaW5nQHNhbHRzdGFjay5jb20+iQE4BBMBAgAiBQJT
        qb6YAhsDBgsJCAcDAgYVCAIJCgsEFgIDAQIeAQIXgAAKCRAOCKFJ3le/vhkqB/0Q
        WzELZf4d87WApzolLG+zpsJKtt/ueXL1W1KA7JILhXB1uyvVORt8uA9FjmE083o1
        yE66wCya7V8hjNn2lkLXboOUd1UTErlRg1GYbIt++VPscTxHxwpjDGxDB1/fiX2o
        nK5SEpuj4IeIPJVE/uLNAwZyfX8DArLVJ5h8lknwiHlQLGlnOu9ulEAejwAKt9CU
        4oYTszYM4xrbtjB/fR+mPnYh2fBoQO4d/NQiejIEyd9IEEMd/03AJQBuMux62tjA
        /NwvQ9eqNgLw9NisFNHRWtP4jhAOsshv1WW+zPzu3ozoO+lLHixUIz7fqRk38q8Q
        9oNR31KvrkSNrFbA3D89uQENBFOpvpgBCADJ79iH10AfAfpTBEQwa6vzUI3Eltqb
        9aZ0xbZV8V/8pnuU7rqM7Z+nJgldibFk4gFG2bHCG1C5aEH/FmcOMvTKDhJSFQUx
        uhgxttMArXm2c22OSy1hpsnVG68G32Nag/QFEJ++3hNnbyGZpHnPiYgej3FrerQJ
        zv456wIsxRDMvJ1NZQB3twoCqwapC6FJE2hukSdWB5yCYpWlZJXBKzlYz/gwD/Fr
        GL578WrLhKw3UvnJmlpqQaDKwmV2s7MsoZogC6wkHE92kGPG2GmoRD3ALjmCvN1E
        PsIsQGnwpcXsRpYVCoW7e2nW4wUf7IkFZ94yOCmUq6WreWI4NggRcFC5ABEBAAGJ
        AR8EGAECAAkFAlOpvpgCGwwACgkQDgihSd5Xv74/NggA08kEdBkiWWwJZUZEy7cK
        WWcgjnRuOHd4rPeT+vQbOWGu6x4bxuVf9aTiYkf7ZjVF2lPn97EXOEGFWPZeZbH4
        vdRFH9jMtP+rrLt6+3c9j0M8SIJYwBL1+CNpEC/BuHj/Ra/cmnG5ZNhYebm76h5f
        T9iPW9fFww36FzFka4VPlvA4oB7ebBtquFg3sdQNU/MmTVV4jPFWXxh4oRDDR+8N
        1bcPnbB11b5ary99F/mqr7RgQ+YFF0uKRE3SKa7a+6cIuHEZ7Za+zhPaQlzAOZlx
        fuBmScum8uQTrEF5+Um5zkwC7EXTdH1co/+/V/fpOtxIg4XO4kcugZefVm5ERfVS
        MA==
        =dtMN
        -----END PGP PUBLIC KEY BLOCK-----

packages:
  - tmux
  - vim


salt_minion:
  conf: 
    master: ${salt_master_address}
    id: ${hostname}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;terraform-module-explained-2&quot;&gt;Terraform module explained&lt;/h3&gt;

&lt;p&gt;Apart from the name, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vm.tf&lt;/code&gt; is only changed in the networking area - we set it back to dhcp.&lt;/p&gt;

&lt;p&gt;Additionally, the cloud config filename changed, but it is almost identical except that it doesn’t install the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;salt-master&lt;/code&gt; package.&lt;/p&gt;

&lt;h3 id=&quot;instantiate-the-salt-minion&quot;&gt;Instantiate the salt minion&lt;/h3&gt;

&lt;p&gt;Now, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terraform init&lt;/code&gt; to bring in the providers, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terraform apply -auto-approve&lt;/code&gt; to bring it up.&lt;/p&gt;

&lt;h3 id=&quot;accept-the-keys-on-the-salt-master&quot;&gt;Accept the keys on the salt master&lt;/h3&gt;

&lt;p&gt;At this stage we have salt master (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;salt-master-01&lt;/code&gt;), which is also a minion, and another minon &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;salt-minion-01&lt;/code&gt;. If we ssh in to the salt master, we can (as root) check the keys pending authorization:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;teacurve@salt-master-01:~$ sudo -i
root@salt-master-01:~# salt-key -F
Local Keys:
master.pem:  23:30:c2:cf:4e:73:a0:db:59:d5:2f:8e:f3:37:fe:a8:54:4b:33:f6:13:8a:05:c9:50:87:e4:ee:42:c5:c6:99
master.pub:  05:f2:ce:6f:22:62:a9:b0:0e:1b:2f:8b:8f:d2:ef:9a:fd:49:c3:0c:9e:a6:6b:4f:b1:38:ec:e2:12:a3:c8:ab
Unaccepted Keys:
salt-master-01:  40:7b:42:3e:c5:42:00:b4:81:8b:b1:75:bd:e7:68:10:9e:c3:a9:08:d0:bc:8f:31:ca:73:84:39:70:ea:fe:09
salt-minion-01:  d5:ee:ee:21:f8:c5:19:10:2b:e9:06:39:3c:87:57:1b:54:22:05:15:cb:48:97:e6:eb:47:98:56:2c:05:68:2b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As part of the security model, salt requires you to acknowledge newly connected minions before it will manage them. They are identified by their minion ID and their public key.&lt;/p&gt;

&lt;p&gt;We can now accept these two pending keys with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@salt-master-01:~# salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
salt-master-01
salt-minion-01
Proceed? [n/Y] y
Key for minion salt-master-01 accepted.
Key for minion salt-minion-01 accepted.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And we can run:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@salt-master-01:~# salt-run manage.status
down:
up:
    - salt-master-01
    - salt-minion-01
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To check the nodes are up and available to be managed by salt.&lt;/p&gt;

&lt;h2 id=&quot;issues&quot;&gt;Issues&lt;/h2&gt;

&lt;p&gt;Unfortunately, proxmox requires us to put the cloud init file on the proxmox server, we cannot pass it through terraform. This means we need to scp it to the right place on the server. I’m not sure if that’s a limitation of the Proxmox API or the telmate/proxmox provider.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linode.com/docs/guides/beginners-guide-to-salt/&quot; target=&quot;_blank&quot;&gt;Beginners guide to Salt (linode)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.saltproject.io/en/latest/topics/tutorials/walkthrough.html&quot; target=&quot;_blank&quot;&gt;Salt in 10 minutes (SaltStack documentation)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://yetiops.net/posts/proxmox-terraform-cloudinit-saltstack-prometheus/&quot; target=&quot;_blank&quot;&gt;Using Terraform and Cloud-Init to deploy and automatically monitor Proxmox instances (yetiops)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloudinit.readthedocs.io/en/latest/topics/examples.html&quot; target=&quot;_blank&quot;&gt;Cloud config examples (cloudinit documentation)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;troubleshooting&quot;&gt;Troubleshooting&lt;/h2&gt;

&lt;h3 id=&quot;debugging-cloud-init&quot;&gt;Debugging cloud-init&lt;/h3&gt;

&lt;p&gt;The previous post showed how to debug Terraform and the telmate/proxmox proivder, but now we also need to be able to debug cloud-init, since we’re giving it a more complicated configuration.&lt;/p&gt;

&lt;p&gt;By default, cloud-init logs to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/var/log/cloud-init-output.log&lt;/code&gt;, and you can review this log after terraform has finished. You can also use cloud-init itself to parse this log more effectively:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/usr/bin/cloud-init analyze show
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;multipart-warning-in-cloud-init-error-logs&quot;&gt;Multipart warning in cloud-init error logs&lt;/h3&gt;

&lt;p&gt;If you get a warning like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__init__.py[WARNING]: Unhandled non-multipart (text/x-not-multipart) userdata: ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check to make sure you cloud-config file starts with the line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#cloud-config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which is required.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Thanks for reading, I hope you found this post useful. In this post I described how to automate the creation of a salt master node via Terraform and cloud-init. We take advantage of the more powerful way to customize cloud-init; by passing our own cloud-init config file that lets us specify more parameters for cloud-init to configure.&lt;/p&gt;

&lt;p&gt;Salt is the configuration management system I have chosen to use to automate my infrastructure. Whilst it is a server-based and agent-based solution, the system is trivial to set up. Additionally, salt can be alternatively run in a &lt;a href=&quot;https://docs.saltproject.io/en/latest/topics/tutorials/quickstart.html&quot; target=&quot;_blank&quot;&gt;‘masterless’&lt;/a&gt; configuration (where the minion is its own master).&lt;/p&gt;

&lt;p&gt;In the next post I will describe how I used salt to configure a reproducable development environment, and some other infrastructure.&lt;/p&gt;</content><author><name>teacurve</name></author><category term="virtualization" /><category term="proxmox" /><category term="cloud-init" /><category term="terraform" /><category term="salt" /><category term="infrastructure" /><category term="automation" /><summary type="html">Introduction</summary></entry><entry><title type="html">Proxmox Automation Part 1: Terraform with Proxmox and cloud-init</title><link href="https://teacurve.github.io/2021/07/17/proxmox-automation-part-1-terraform.html" rel="alternate" type="text/html" title="Proxmox Automation Part 1: Terraform with Proxmox and cloud-init" /><published>2021-07-17T21:10:00+00:00</published><updated>2021-07-17T21:10:00+00:00</updated><id>https://teacurve.github.io/2021/07/17/proxmox-automation-part-1-terraform</id><content type="html" xml:base="https://teacurve.github.io/2021/07/17/proxmox-automation-part-1-terraform.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.terraform.io/&quot; target=&quot;_blank&quot;&gt;Terraform&lt;/a&gt; is an infrastructure-as-code system that allows you to define an environment in a text file, and repeatably bring that environment to life on a variety of platforms (AWS, Azure, etc).&lt;/p&gt;

&lt;p&gt;I’ve tried out several different virtualization providers, but recently I have been running &lt;a href=&quot;https://pve.proxmox.com/wiki/Main_Page&quot; target=&quot;_blank&quot;&gt;Proxmox&lt;/a&gt; (based on QEMU/KVM) in my home lab. I think Proxmox is fantastic for a homelab; it has a LOT of really advanced features (you can send a vm between clustered nodes, and it actually works), a usable web interface, and even a REST API. For enterprise use it’s a little rough around the edges (at least the community version is), but for me it’s great.&lt;/p&gt;

&lt;p&gt;I don’t really do much ‘devops’ but I do like repeatable environments. Normally I will just manually create VMs for research (or hosting services at home), but I document here how I got Terraform working with Proxmox to automate the creation of a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cloud-init&lt;/code&gt; Ubuntu Server VM.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Just so we’re on the same page, here are the main components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Terraform - An infrastructure-as-code system&lt;/li&gt;
  &lt;li&gt;Proxmox - A virtualization server (like ESXi)&lt;/li&gt;
  &lt;li&gt;cloud-init - A standardized way of customizing cloud instances (basically how we specify some things about a new instance from a template)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In our case, we’ll use Ubuntu as the OS for the new instance. I also happened to use Ubuntu as the machine from which I used Terraform, but Terraform runs on other operating systems, including Windows.&lt;/p&gt;

&lt;p&gt;I have looked at various tutorials and sources of documentation, and provided a list in &lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt; of some that might be helpful. I’ve also compiled a list of issues that I ran in to in the &lt;a href=&quot;#troubleshooting&quot;&gt;Troubleshooting&lt;/a&gt; Section, which you should check if you have problems following.&lt;/p&gt;

&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;

&lt;h3 id=&quot;step-1---create-a-cloud-init-template&quot;&gt;Step 1 - Create a cloud-init template&lt;/h3&gt;

&lt;p&gt;There are essentially two ways of doing this. The first is to create a ‘golden image’ template of the OS you want, configure it how you like, and then convert to a template. Not only is this laborious for multiple templates, but you can run into issues if you do not prep the image correctly.&lt;/p&gt;

&lt;h4 id=&quot;get-the-image&quot;&gt;Get the image&lt;/h4&gt;

&lt;p&gt;Luckily, since everyone uses the cloud now, many operating systems provide ‘cloud-ready’ images for us to use. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cloud-init&lt;/code&gt; is the standard way of doing this. In my case, I wanted a Ubuntu Server template, and they do provide cloud-ready images, which you can browse &lt;a href=&quot;https://cloud-images.ubuntu.co&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. So, on the Proxmox server, download the chosen image:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget wget https://cloud-images.ubuntu.com/focal/current/focal-server=cloudimg-amd64.img
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;prep-the-image&quot;&gt;Prep the image&lt;/h4&gt;

&lt;p&gt;Since proxmox is KVM/QEMU based, it’s very helpful to have the QEMU guest agent installed in the image. In fact, I found not having it caused issues with the telmate/proxmox provider. Of course, the Proxmox wiki has a page on it &lt;a href=&quot;https://pve.proxmox.com/wiki/Qemu-guest-agent&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There’s a really cool project called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libguestfs&lt;/code&gt; that contains tools for accessing and modifying disk images like ours. Install it with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apt install libguestfs-tools
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then make a copy of the image file (just in case…):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cp focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64-agent.img
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then slipstream the agent into it:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virt-customize -a focal-server-cloudimg-amd64-agent.img --install qemu-guest-agent
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you leave the image like this everything will work, but clones will have the same &lt;a href=&quot;https://www.freedesktop.org/software/systemd/man/machine-id.html&quot; target=&quot;_blank&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/etc/machine-id&lt;/code&gt;&lt;/a&gt;. This can mess up all sorts of things, including DHCP - you can get &lt;a href=&quot;https://techblog.jeppson.org/2020/05/ubuntu-20-04-cloned-vm-same-dhcp-ip-fix/&quot; target=&quot;_blank&quot;&gt;duplicate addresses handed out&lt;/a&gt;. To fix this, you also need to run:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;virt-sysprep --operations machine-id -a focal-server-cloudimg-amd64-agent.img
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which will reset the machine-id.&lt;/p&gt;

&lt;p&gt;We now have a ubuntu server cloud-ready image with qemu-guest-agent present.&lt;/p&gt;

&lt;h4 id=&quot;templace-creation-script&quot;&gt;Templace creation script&lt;/h4&gt;

&lt;p&gt;I created a script that does the above for you:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#!/bin/bash

wget https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.img

next_id=`pvesh get /cluster/nextid`

qm create $next_id --memory 2048 --net0 virtio,bridge=vmbr0

cp focal-server-cloudimg-amd64.img focal-server-cloudimg-amd64-agent.img
virt-customize -a focal-server-cloudimg-amd64-agent.img --install qemu-guest-agent
virt-sysprep --operations machine-id -a focal-server-cloudimg-amd64-agent.img
qm importdisk $next_id focal-server-cloudimg-amd64-agent.img local-lvm
qm set $next_id --scsihw virtio-scsi-pci --scsi0 &quot;local-lvm:vm-$next_id-disk-0&quot;

qm set $next_id --ide2 local-lvm:cloudinit
qm set $next_id -boot c --bootdisk scsi0
qm set $next_id --agent enabled=1
qm set $next_id --name &quot;ubuntu-cloudready-template&quot;
qm template $next_id
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;create-the-vm-template&quot;&gt;Create the VM Template&lt;/h4&gt;

&lt;p&gt;First, get the next available VM ID:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pvesh get /cluster/nextid
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, create a new VM with that ID:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qm create 138 --name &quot;ubuntu-cloudready-template&quot; --memory 2048 --net0 virtio,bridge=vmbr0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You want to have minimum specs, but that will depend on the OS. You’re also free to change the bridge, or any other settings. But all of it is modifiable for new instances, so it doesn’t matter too much.&lt;/p&gt;

&lt;p&gt;Import the image we modified, and set it as the vm disk:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qm importdisk 138 focal-server-cloudimg-amd64-agent.img local-lvm
qm set 138 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-138-disk-0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, attach the cloud init disk (a magical auto-generated cd-rom device):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qm set 138 --ide2 local-lvm:cloudinit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set the boot disk to be the new one:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qm set 138 -boot c --bootdisk scsi0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Enable qemu-agent:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qm set 138 --agent enabled=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, make the VM as a template. Whilst not strictly necessary, it speeds up clone times.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qm template 138
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you look in your Proxmox GUI, you’ll find a new template VM in there.&lt;/p&gt;

&lt;h3 id=&quot;step-2---basic-terraform-configuration&quot;&gt;Step 2 - Basic Terraform Configuration&lt;/h3&gt;

&lt;p&gt;Now, you’ll need to &lt;a href=&quot;https://learn.hashicorp.com/tutorials/terraform/install-cli&quot; target=&quot;_blank&quot;&gt;install Terraform&lt;/a&gt; according to their docs. I installed the autocomplete too.&lt;/p&gt;

&lt;p&gt;Once you have the Terraform CLI working, create a folder for your terraform ‘code’:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkdir terraform &amp;amp;&amp;amp; cd terraform&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;secrets&quot;&gt;Secrets&lt;/h4&gt;

&lt;p&gt;You’ll need to specify credentials for Terraform to use to interact with the Proxmox API. You can either use a username/password, or an API token. It’s simple to do either way, but I used a token.&lt;/p&gt;

&lt;p&gt;One of the great things about Proxmox is its useful wiki. The relevant pages are &lt;a href=&quot;https://pve.proxmox.com/wiki/Proxmox_VE_API&quot; target=&quot;_blank&quot;&gt;Proxmox VE API&lt;/a&gt; and &lt;a href=&quot;https://pve.proxmox.com/wiki/User_Management#pveum_tokens&quot; target=&quot;_blank&quot;&gt;User Management&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To create an API token, go to Datacenter -&amp;gt; Permissions -&amp;gt; API Tokens -&amp;gt; Add. Because I’m lazy, I created it for the root user and &lt;em&gt;unchecked Privilege Separation&lt;/em&gt;. This means the token has access to everything th root user does. In the future, I may go back and try and figure out what the least privilege that I really need is, but for now, since I’m the only user of the server, it’s fine.&lt;/p&gt;

&lt;p&gt;Like all code automation, secrets are difficult to handle. You can do some fancier things with Vaults, but for my purposes it is enough to have them in environment variables. To do this, export the variables in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export PM_API_TOKEN_ID=&quot;root@pam!terraform&quot;
export PM_API_TOKEN_SECRET=&quot;&amp;lt;secret&amp;gt;&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And don’t forget to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source ~/.bashrc&lt;/code&gt; afterwards.&lt;/p&gt;

&lt;h4 id=&quot;providers&quot;&gt;Providers&lt;/h4&gt;

&lt;p&gt;Terraform has a plugin-architecture where ‘Providers’ can be added to interact with different backends. Most people use Terraform with things like Heroku or AWS, but there is a third party provider for Proxmox; &lt;a href=&quot;https://github.com/Telmate/terraform-provider-proxmox&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;telmate/proxmox&lt;/code&gt;&lt;/a&gt;. Terraform makes installing new providers easy - if you declare a provider is required, it will install it for you when running the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Terraform init&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;Create a file named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.tf&lt;/code&gt; with the following contents:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# PM_API_TOKEN_SECRET 
# PM_API_TOKEN_ID 
# (in .bashrc)

terraform {
  required_providers {
    proxmox = {
      source = &quot;telmate/proxmox&quot;
      version = &quot;2.7.4&quot;
    }
  }
}


provider &quot;proxmox&quot; {
    pm_api_url = &quot;https://&amp;lt;proxmox_ip&amp;gt;:8006/api2/json&quot;
    pm_tls_insecure = &quot;true&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The version is the latest as of writing, I don’t know how to make this automatically update (using the string ‘latest’ didn’t work).&lt;/p&gt;

&lt;p&gt;This file specifies that a provider &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;telmate/proxmox&lt;/code&gt; should be installed, and points it at our Proxmox server. If you have proper SSL certificate trust set up, by all means change &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pm_tls_insecure&lt;/code&gt;, but I need it since it’s just a self-signed certificate.&lt;/p&gt;

&lt;p&gt;Now, from this directory run&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;terraform init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And terraform will download and install the plugin for you.&lt;/p&gt;

&lt;h4 id=&quot;test-proxmox-works&quot;&gt;Test Proxmox works&lt;/h4&gt;

&lt;p&gt;There doesn’t seem to be a good way to test that the proxmox provider can actually access the Proxmox server. But, if you enable debug logging and have a bare minimum proxmox resource, you can check the debug log and ensure you could get to it.&lt;/p&gt;

&lt;p&gt;First, enable debug as per the &lt;a href=&quot;#add-logging&quot;&gt;Add Logging&lt;/a&gt; section of troubleshooting.&lt;/p&gt;

&lt;p&gt;Then, create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vm.tf&lt;/code&gt; file, and populate with the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;resource &quot;proxmox_vm_qemu&quot; &quot;terravm01&quot; {
        name = &quot;terravm&quot;
        target_node = &quot;hades&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note; you’ll need to change the target_node to be whatever Proxmox node you want to use.&lt;/p&gt;

&lt;p&gt;Now, run terraform with:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;terraform apply
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will fail, but if you look inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terraform-plugin-proxmox.log&lt;/code&gt; file, if it was successful you should see requests and replies from the server, at which point you know it’s working, and your credentials are correct.&lt;/p&gt;

&lt;h3 id=&quot;step-3---proxmox-terraform-time&quot;&gt;Step 3 - Proxmox Terraform time&lt;/h3&gt;

&lt;p&gt;At this stage we have a proxmox VM Template available to clone from, and we have a basic terraform template. Now we need to expand it to define the new instance we want to create.&lt;/p&gt;

&lt;h4 id=&quot;describe-the-instance&quot;&gt;Describe the instance&lt;/h4&gt;

&lt;p&gt;Create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vm.tf&lt;/code&gt; in the terraform folder (replace the contents if you created it in a previous step). This is the file in which we will define our instance. Terraform basically concatenates all the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tf&lt;/code&gt; files in a directory together (without recursing), so you can actually call this file whatever you want.&lt;/p&gt;

&lt;p&gt;Here’s my complete &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vm.tf&lt;/code&gt; file, which I’ll go through in detail below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;resource &quot;proxmox_vm_qemu&quot; &quot;terravm01&quot; {
  
    name = &quot;terravm01&quot;
    target_node = &quot;hades&quot;
    clone = &quot;ubuntu-cloudready-template&quot;
    os_type = &quot;cloud-init&quot;
    balloon = 1024
    boot = &quot;order=scsi0&quot;

    agent = 1

    cores = 2
    sockets = 1
    memory = 2560
    
    disk {  
        size            = &quot;10G&quot;
        type            = &quot;scsi&quot;
        storage         = &quot;local-lvm&quot;
    }
    
      vga {
        type = &quot;std&quot;
      }


      # Set the network
      network {
          model = &quot;virtio&quot;
          bridge = &quot;vmbr0&quot;
      }

    ipconfig0 = &quot;ip=dhcp&quot;
    sshkeys = &quot;${file(&quot;~/.ssh/id_rsa.pub&quot;)}&quot;

    # Ignore changes to the network
    ## MAC address is generated on every apply, causing
    ## TF to think this needs to be rebuilt on every apply
    lifecycle {
        ignore_changes = [
            network
        ]
    }

    connection {
        type = &quot;ssh&quot;
        user = &quot;${self.ssh_user}&quot;
        private_key = &quot;${file(&quot;~/.ssh/id_rsa&quot;)}&quot;
        host = &quot;${self.ssh_host}&quot;
        port = &quot;${self.ssh_port}&quot;
    }

    provisioner &quot;remote-exec&quot; {
    inline = [  
      // here you will see the actual ip address        
      &quot;/sbin/ip a&quot;  
    ]      
  }

}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;resource &quot;proxmox_vm_qemu&quot; &quot;terravm01&quot; {
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Defines the resource named ‘terravm01’ which is of type ‘proxmox_vm_qemu’ - this type is defined by the telmate/proxmox provider we use. The name is irrelevant outside of the tf script.&lt;/p&gt;

&lt;p&gt;The first big chunk define some basic properties about the VM:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name = &quot;terravm01&quot;
target_node = &quot;hades&quot;
clone = &quot;ubuntu-cloudready-template&quot;
os_type = &quot;cloud-init&quot;
balloon = 1024
boot = &quot;order=scsi0&quot;

agent = 1

cores = 2
sockets = 1
memory = 2560
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Most importantly we define the name (terravm01), the target node, and the vm we are cloning from (The ‘ubuntu-cloudready-template’ template we created previously).&lt;/p&gt;

&lt;p&gt;The os_type is special (cloud-init), balloon defines the minimum memory to give to the VM, and we specify the boot order just incase we forget to do it properly in the template.&lt;/p&gt;

&lt;p&gt;We also specified the agent is enabled, and give the VM 2 cores (1 socket) and 2.5 gigs of max memory (for it to balloon into).&lt;/p&gt;

&lt;p&gt;Next comes the disk block:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;disk {  
        size            = &quot;10G&quot;
        type            = &quot;scsi&quot;
        storage         = &quot;local-lvm&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where we define the disk for the VM. We give it 10 gigs, because it doesn’t need to be big.&lt;/p&gt;

&lt;p&gt;The next blog ‘vga’ seems to be required, and caused an error if I didn’t include it. I just set it as the standard.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vga {
  type = &quot;std&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The network block defines the network adapter for the instance. You can modify this as you wish (for example, if you’re creating an entire isolated network of machines).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;network {
    model = &quot;virtio&quot;
    bridge = &quot;vmbr0&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the next section we set some cloud-init properties for the proxmox VM. There are a few ways in which you can pass cloud-init properties through, I chose the easiest way. There is a more powerful method, in which you send over an actual cloud-init config file to the proxmox server for the instance. For now I really only want to configure the network and sshkey for the user.&lt;/p&gt;

&lt;p&gt;Here, I read the ssh public key from a local file with some dynamic code, but you can put the actual key in there too, if you prefer. The public key will be put into the ubuntu user’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.ssh/authorized_keys&lt;/code&gt; file.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh_user = &quot;ubuntu&quot;
ipconfig0 = &quot;ip=dhcp&quot;
sshkeys = &quot;${file(&quot;~/.ssh/id_rsa.pub&quot;)}&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The lifecycle block is as the comment describes - a workaround to stop the recreation of the instance every time you do &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terraform apply&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The next block describes how Terraform can connect to the instance after creation:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;connection {
        type = &quot;ssh&quot;
        user = &quot;${self.ssh_user}&quot;
        private_key = &quot;${file(&quot;~/.ssh/id_rsa&quot;)}&quot;
        host = &quot;${self.ssh_host}&quot;
        port = &quot;${self.ssh_port}&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Mostly self explanatory, the private_key corresponds to the public key from the earlier &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sshkey&lt;/code&gt; variable.&lt;/p&gt;

&lt;p&gt;Finally, the remote exec provisioner block:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;provisioner &quot;remote-exec&quot; {
    inline = [  
      // here you will see the actual ip address        
      &quot;/sbin/ip a&quot;  
    ]   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This connects to the instance and dumps its ip address, which makes sure everything is working (and displays the ip we need to connect to)&lt;/p&gt;

&lt;h4 id=&quot;instantiate&quot;&gt;Instantiate!&lt;/h4&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;terraform apply&lt;/code&gt; to start up the instance. If you have errors in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tf&lt;/code&gt; you can correct them and try again, if you have issues consult the &lt;a href=&quot;#troubleshooting&quot;&gt;Troublshooting&lt;/a&gt; section below.&lt;/p&gt;

&lt;p&gt;If all goes to plan, you should see output like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  Enter a value: yes

proxmox_vm_qemu.terravm01: Destroying... [id=hades/qemu/137]
proxmox_vm_qemu.terravm01: Destruction complete after 4s
proxmox_vm_qemu.terravm01: Creating...
proxmox_vm_qemu.terravm01: Still creating... [10s elapsed]
proxmox_vm_qemu.terravm01: Still creating... [20s elapsed]
proxmox_vm_qemu.terravm01: Still creating... [30s elapsed]
proxmox_vm_qemu.terravm01: Still creating... [40s elapsed]
proxmox_vm_qemu.terravm01: Still creating... [50s elapsed]
proxmox_vm_qemu.terravm01: Still creating... [1m0s elapsed]
proxmox_vm_qemu.terravm01: Provisioning with 'remote-exec'...
proxmox_vm_qemu.terravm01 (remote-exec): Connecting to remote host via SSH...
proxmox_vm_qemu.terravm01 (remote-exec):   Host: 192.168.1.116
proxmox_vm_qemu.terravm01 (remote-exec):   User: ubuntu
proxmox_vm_qemu.terravm01 (remote-exec):   Password: false
proxmox_vm_qemu.terravm01 (remote-exec):   Private key: true
proxmox_vm_qemu.terravm01 (remote-exec):   Certificate: false
proxmox_vm_qemu.terravm01 (remote-exec):   SSH Agent: false
proxmox_vm_qemu.terravm01 (remote-exec):   Checking Host Key: false
proxmox_vm_qemu.terravm01 (remote-exec):   Target Platform: unix
proxmox_vm_qemu.terravm01 (remote-exec): Connected!
proxmox_vm_qemu.terravm01 (remote-exec): 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
proxmox_vm_qemu.terravm01 (remote-exec):     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
proxmox_vm_qemu.terravm01 (remote-exec):     inet 127.0.0.1/8 scope host lo
proxmox_vm_qemu.terravm01 (remote-exec):        valid_lft forever preferred_lft forever
proxmox_vm_qemu.terravm01 (remote-exec):     inet6 ::1/128 scope host
proxmox_vm_qemu.terravm01 (remote-exec):        valid_lft forever preferred_lft forever
proxmox_vm_qemu.terravm01 (remote-exec): 2: eth0: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP&amp;gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
proxmox_vm_qemu.terravm01 (remote-exec):     link/ether be:3e:70:fc:eb:28 brd ff:ff:ff:ff:ff:ff
proxmox_vm_qemu.terravm01 (remote-exec):     inet 192.168.1.116/24 brd 192.168.1.255 scope global dynamic eth0
proxmox_vm_qemu.terravm01 (remote-exec):        valid_lft 86394sec preferred_lft 86394sec
proxmox_vm_qemu.terravm01 (remote-exec):     inet6 fe80::bc3e:70ff:fefc:eb28/64 scope link
proxmox_vm_qemu.terravm01 (remote-exec):        valid_lft forever preferred_lft forever
proxmox_vm_qemu.terravm01: Creation complete after 1m5s [id=hades/qemu/137]

Apply complete! Resources: 1 added, 0 changed, 1 destroyed.
manager@linux-manager:~/terraform$ 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And be able to log in to the new machine:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;manager@linux-manager:~/terraform$ ssh ubuntu@192.168.1.116
Welcome to Ubuntu 20.04.2 LTS (GNU/Linux 5.4.0-77-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Sun Jul 18 00:48:37 UTC 2021

  System load:  0.18              Processes:             122
  Usage of /:   15.1% of 9.52GB   Users logged in:       0
  Memory usage: 14%               IPv4 address for eth0: 192.168.1.116
  Swap usage:   0%


0 updates can be applied immediately.


Last login: Sun Jul 18 00:47:16 2021 from 192.168.1.136
To run a command as administrator (user &quot;root&quot;), use &quot;sudo &amp;lt;command&amp;gt;&quot;.
See &quot;man sudo_root&quot; for details.

ubuntu@terravm01:~$ 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Cool!&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pve.proxmox.com/wiki/Cloud-Init_Support&quot; target=&quot;_blank&quot;&gt;Cloud-init support (Proxmox Wiki)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Telmate/terraform-provider-proxmox/blob/master/docs/guides/cloud_init.md&quot; target=&quot;_blank&quot;&gt;Telmate/Proxmox Cloud-init guide (telmate)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://vectops.com/2020/05/provision-proxmox-vms-with-terraform-quick-and-easy/&quot; target=&quot;_blank&quot;&gt;Provision Proxmox VMs with Terraform Quick and Easy (vectops)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;troubleshooting&quot;&gt;Troubleshooting&lt;/h2&gt;

&lt;h3 id=&quot;add-logging&quot;&gt;Add Logging&lt;/h3&gt;

&lt;p&gt;You can increase logging output for both Terraform and the Proxmox provider. To debug Terraform issues, export the log level before running the terraform command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export TF_LOG=trace
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See the available log levels &lt;a href=&quot;https://www.terraform.io/docs/cli/config/environment-variables.html&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To enable debug logging in the provider, use the following provider block in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.tf&lt;/code&gt; which specifies a log file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;provider &quot;proxmox&quot; {
  pm_api_url = &quot;https://&amp;lt;proxmox ip&amp;gt;:8006/api2/json&quot;
  pm_tls_insecure = &quot;true&quot;
  pm_log_enable = true
  pm_log_file = &quot;terraform-plugin-proxmox.log&quot;
  pm_log_levels = {
    _default = &quot;debug&quot;
    _capturelog = &quot;&quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;invalid-boot-disk&quot;&gt;Invalid boot disk&lt;/h3&gt;

&lt;p&gt;First, Make sure you downloaded the right img file. For the focal image, the one with ‘disk’ in the name will not boot. Second, check you set the disk image to the template after importing it like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qm set 138 --scsihw virtio-scsi-pci --scsi0 &quot;local-lvm:vm-138-disk-0&quot;     
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you didn’t, it will show up as ‘unused’ in the VM template (and instance).&lt;/p&gt;

&lt;h3 id=&quot;slow-vm-creation-10-minutes-or-dont-get-dhcp-ip-output&quot;&gt;Slow vm creation (10+ minutes) or don’t get dhcp IP output&lt;/h3&gt;

&lt;p&gt;Make sure you add the qemu agent to the template with virt-customize, and make sure you enable it in the template (and in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tf&lt;/code&gt; file might be necessary too, not sure).&lt;/p&gt;

&lt;h3 id=&quot;terraform-plugin-crash&quot;&gt;Terraform plugin crash&lt;/h3&gt;

&lt;p&gt;I had the plugin crash in:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;goroutine 38 [running]:
github.com/Telmate/proxmox-api-go/proxmox.(*Client).GetVmRefsByName(0xc0004b0280, 0xc00032eb80, 0x9, 0xb9c6a0, 0xd82630, 0x0, 0x0, 0xc000209600)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check your logs for something like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;2021/07/18 00:52:43 [DEBUG] checking for duplicate name
2021/07/18 00:52:43 &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; REQUEST:
2021/07/18 00:52:48 &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; REQUEST:
2021/07/18 00:52:53 &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; REQUEST:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In my case, it was because I had a typo in the proxmox url. Sure, the plugin have a better error, but we should probably give it proper urls :)&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I hope you found this blog post useful. This is the first in a series on Proxmox automation. Here I described how to get up and running with Terraform and Proxmox, and we created a cloud-init Ubuntu Server templated and instantiated a clone of it automatically with different vm settings.&lt;/p&gt;

&lt;p&gt;In subsequent posts I plan extend this to include installing and configuring software on the new instances, and also to create more advanced networks of machines (like a kubernetes cluster, or a windows Active Directory domain).&lt;/p&gt;</content><author><name>teacurve</name></author><category term="virtualization" /><category term="proxmox" /><category term="cloud-init" /><category term="terraform" /><category term="infrastructure" /><category term="automation" /><summary type="html">Introduction</summary></entry></feed>